# Real-Time Face Recognition & Emotion Detection System

A production-ready real-time application combining **face recognition** and **emotion detection** with advanced temporal smoothing techniques. Built with Python, InsightFace, PyTorch, and Streamlit.

This project demonstrates end-to-end AI/ML development: from custom CNN training with advanced techniques (SE-ResNet, Focal Loss, OneCycleLR) to real-time inference with sophisticated smoothing algorithms for stable predictions.

## ÔøΩ Demo

https://github.com/user-attachments/assets/video_demo_emotion.mp4

https://github.com/user-attachments/assets/video_demo_emotion.mp4

> **Demo Video**: Real-time face recognition and emotion detection in action. The system detects faces, recognizes enrolled users, and classifies emotions with live probability visualization and change history tracking.

*If the video doesn't play above, you can find it in the [videos/video_demo_emotion.mp4](videos/video_demo_emotion.mp4) folder.*

## ÔøΩüéØ Project Overview

This system performs two main tasks simultaneously:
1. **Face Recognition**: Identify individuals using InsightFace embeddings
2. **Emotion Classification**: Detect 7 emotions (happy, sad, angry, surprise, fear, disgust, neutral) using a custom-trained ResNet with SE attention blocks

**Key Innovation**: Multi-stage temporal smoothing pipeline that eliminates prediction flickering while maintaining fast response to genuine emotion changes.

## ‚ú® Features

### Face Recognition
- **Real-time Face Recognition**: Detect and recognize faces using InsightFace with 512-dimensional embeddings
- **User Enrollment**: Multi-image enrollment system for robust recognition
- **High Accuracy**: Buffalo_s/buffalo_l models with cosine similarity matching
- **Multi-face Support**: Simultaneous detection and recognition of multiple faces
- **Persistent Storage**: Face embeddings saved as NumPy arrays

### Emotion Detection
- **7-Class Emotion Recognition**: Happy, Sad, Angry, Surprise, Fear, Disgust, Neutral
- **Custom ResNet Model**: SE-ResNet architecture with channel attention mechanisms
- **Advanced Training**: Focal Loss, Label Smoothing, OneCycleLR scheduler, data augmentation
- **Temporal Smoothing**: Three-tier smoothing system for stable predictions:
  - **PredictionBuffer**: Multi-frame temporal aggregation with adaptive sampling (SNAP/TRANS/FML modes)
  - **EMA Smoothing**: Exponential moving average with confidence-based adaptation
  - **Smart Switching**: State machine prevents rapid flickering between emotions
- **Real-time Performance**: Optimized TorchScript model running at 10 FPS

### User Experience
- **Web Interface**: Clean Streamlit-based UI with live video streaming
- **Visual Feedback**: Bounding boxes, confidence scores, emotion badges
- **Configurable**: Adjustable thresholds, intervals, and smoothing parameters

## üõ† Technology Stack

### Core Frameworks
- **InsightFace** (0.7.3): State-of-the-art face detection and recognition
- **PyTorch** (2.x): Deep learning framework for custom emotion model
- **Streamlit**: Interactive web interface
- **streamlit-webrtc** (0.48.0): Real-time WebRTC video streaming

### Computer Vision & ML
- **OpenCV**: Image processing and transformations
- **ONNX Runtime**: Optimized inference for InsightFace models
- **NumPy**: Numerical operations and embedding storage
- **TorchScript**: Optimized PyTorch model serialization

### Model Architecture
- **Face Recognition**: InsightFace ArcFace embeddings (buffalo_s/buffalo_l)
- **Emotion Detection**: Custom SE-ResNet with:
  - Squeeze-and-Excitation (SE) blocks for channel attention
  - Residual connections for gradient flow
  - Global Average Pooling
  - Dropout regularization

### Training Techniques
- **Loss Function**: Focal Loss with label smoothing (handles class imbalance)
- **Optimizer**: AdamW with weight decay
- **Scheduler**: OneCycleLR for fast convergence
- **Data Augmentation**: 
  - Random horizontal flip, rotation, affine transforms
  - Gaussian blur, sharpness adjustment, autocontrast
  - Random erasing (CutOut)
- **Regularization**: Dropout, batch normalization, weight decay

## üìÅ Project Structure

```
face-emotion/
‚îú‚îÄ‚îÄ app.py                      # Main Streamlit application
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ README.md                   # This file
‚îú‚îÄ‚îÄ LICENSE.md                  # MIT License
‚îÇ
‚îú‚îÄ‚îÄ models/                     # Trained models
‚îÇ   ‚îú‚îÄ‚îÄ emotion_resnet_model_ts.pth    # TorchScript emotion model
‚îÇ   ‚îî‚îÄ‚îÄ ...                     # InsightFace models (auto-downloaded)
‚îÇ
‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îî‚îÄ‚îÄ embeddings/             # Face embeddings storage (*.npy files)
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ Emotion.ipynb           # Model training & experimentation
‚îÇ
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ face_engine.py          # Face recognition engine (InsightFace wrapper)
    ‚îú‚îÄ‚îÄ emotion_classifier.py   # Emotion model loader & inference
    ‚îú‚îÄ‚îÄ emotion_ema.py          # EMA smoothing with adaptive alpha
    ‚îú‚îÄ‚îÄ emotion_infer.py        # Face cropping utilities
    ‚îú‚îÄ‚îÄ emotion_ui.py           # UI rendering (badges, overlays)
    ‚îÇ
    ‚îú‚îÄ‚îÄ model/
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îî‚îÄ‚îÄ recognition_result.py   # Face recognition result dataclass
    ‚îÇ
    ‚îî‚îÄ‚îÄ utils/
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îî‚îÄ‚îÄ util.py             # Math utilities (cosine similarity, sigmoid)
```

## üß† Technical Deep Dive

### Emotion Detection Pipeline

The emotion detection system uses a sophisticated multi-stage pipeline to achieve stable, accurate predictions:

#### 1. Model Architecture: SE-ResNet

```python
class ImprovedResNet:
    - Stem: 2 √ó Conv3x3(64) layers
    - Layer1: 2 √ó ResBlockSE(64‚Üí64, stride=1)
    - Layer2: 2 √ó ResBlockSE(64‚Üí128, stride=2)
    - Layer3: 2 √ó ResBlockSE(128‚Üí256, stride=2)
    - Layer4: 2 √ó ResBlockSE(256‚Üí512, stride=2)
    - Global Average Pooling
    - Classifier: Dropout(0.5) ‚Üí Linear(512‚Üí256) ‚Üí ReLU ‚Üí Dropout(0.3) ‚Üí Linear(256‚Üí7)
```

**SE (Squeeze-and-Excitation) Blocks**: Channel attention mechanism that learns to emphasize important features:
- Squeeze: Global Average Pooling ‚Üí 1D vector
- Excitation: FC(reduction=32) ‚Üí ReLU ‚Üí FC ‚Üí Sigmoid
- Scale: Multiply input by channel weights

**Benefits**: +2-3% accuracy improvement, helps model focus on discriminative facial features.

#### 2. Training Process

**Dataset**: FER2013 (Face Expression Recognition)
- 28,709 training images
- 3,589 validation images
- 7 classes: angry, disgust, fear, happy, neutral, sad, surprise
- 48√ó48 grayscale images

**Training Configuration**:
```python
# Loss: Focal Loss + Label Smoothing
loss = FocalLoss(
    alpha=class_weights,     # Handle class imbalance
    gamma=2.0,               # Focus on hard examples
    label_smoothing=0.1      # Prevent overconfidence
)

# Optimizer: AdamW with weight decay
optimizer = AdamW(lr=1e-3, weight_decay=0.05)

# Scheduler: OneCycleLR (fast convergence)
scheduler = OneCycleLR(
    max_lr=1e-3,
    epochs=25,
    pct_start=0.1,           # 10% warmup
    anneal_strategy='cos'    # Cosine annealing
)

# Data Augmentation
- RandomHorizontalFlip(p=0.5)
- RandomRotation(15¬∞)
- RandomAffine(translate=0.1, scale=0.9-1.1)
- GaussianBlur, Sharpness, Autocontrast
- RandomErasing (CutOut)
```

**Results**: ~65-70% validation accuracy (typical for FER2013 with custom models)

**Key Challenge**: Model confusion between angry/sad/neutral (low inter-class gap: 0.02-0.20)

#### 3. Temporal Smoothing Architecture

To solve the flickering problem caused by model uncertainty, we implemented a three-tier smoothing system:

##### Tier 1: PredictionBuffer (Multi-frame Aggregation)

Stores last N predictions with adaptive sampling based on confidence gap:

```python
class PredictionBuffer:
    def __init__(self, maxlen=3):
        self.buffer = deque(maxlen=maxlen)  # Circular buffer
    
    def aggregate(self, gap):
        if gap >= 0.35:
            # SNAP mode: High confidence, use 2 most recent
            return mean(buffer[-2:])
        elif gap >= 0.10:
            # TRANS mode: Transition, use recent half
            return mean(buffer[len//2:])
        else:
            # FML mode: Low confidence, use First-Middle-Last
            return mean([first, middle, last])
```

**Rationale**:
- High confidence (gap ‚â• 0.35): Fast response, use recent frames
- Medium confidence (gap ‚â• 0.10): Transition state, favor recent
- Low confidence (gap < 0.10): Uncertain, sample diverse frames (FML)

##### Tier 2: EMA (Exponential Moving Average)

Applies second-stage smoothing with adaptive alpha based on confidence:

```python
class EmotionEMA:
    def update(self, probs, gap):
        base_alpha = 0.5
        if gap >= 0.20:
            adaptive_alpha = min(base_alpha * 1.5, 0.9)  # Faster update
        else:
            adaptive_alpha = max(base_alpha * 0.5, 0.1)  # Slower update
        
        self.ema = (1 - adaptive_alpha) * self.ema + adaptive_alpha * probs
```

**Benefits**: Smooth transitions, adaptive response speed

##### Tier 3: State Machine (Smart Switching)

Prevents rapid emotion changes with hysteresis logic:

```python
def should_switch_emotion(current, new, gap, streak):
    # Require 3 consecutive frames OR very high confidence
    if streak >= 3:
        return True
    if gap >= 0.35 and snap_on_strong_new:  # Immediate snap
        return True
    return False
```

**Result**: Eliminates flickering while maintaining ~1s response time for genuine changes

### Performance Optimizations

1. **TorchScript**: Model compiled to TorchScript for 20-30% faster inference
2. **Async Inference**: 10 FPS processing (0.1s interval) with last-frame retention
3. **Non-blocking Transfers**: `to(device, non_blocking=True)` for GPU
4. **Batch Normalization**: Fused BN layers for faster forward pass
5. **Mixed Precision**: AMP (Automatic Mixed Precision) during training

### Model Performance Analysis

**Confusion Matrix Insights**:
- ‚úÖ **Happy**: 0.8-0.9 confidence gap (excellent)
- ‚ö†Ô∏è **Angry/Sad/Neutral**: 0.02-0.20 gap (high confusion)
- ‚úÖ **Surprise**: 0.3-0.4 gap (good)
- ‚ö†Ô∏è **Fear**: Often confused with surprise

**Smoothing Impact**:
- Without smoothing: ~5-10 flickers per second
- With Buffer only: ~2-3 flickers per second
- With Buffer + EMA: <1 flicker per 5 seconds
- With full pipeline: Stable predictions, ~1s transition time



## üöÄ Quick Start

### Prerequisites

- Python 3.10+ (tested with 3.10 and 3.12)
- Webcam for real-time detection
- Modern browser with WebRTC support (Chrome/Firefox/Edge recommended)
- **Pre-trained emotion model**: Download `emotion_resnet_model_ts.pth` or train your own (see Training section)

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/yourusername/face-emotion.git
   cd face-emotion
   ```

2. **Create virtual environment**:
   ```bash
   # Using uv (recommended - faster)
   uv venv uv_venv --python 3.10
   source uv_venv/bin/activate  # Windows: uv_venv\Scripts\activate
   
   # Or using standard venv
   python3 -m venv venv
   source venv/bin/activate  # Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   # Using uv
   uv pip install -r requirements.txt
   
   # Or standard pip
   pip install -r requirements.txt
   ```

4. **Download/Place Emotion Model**:
   - Place `emotion_resnet_model_ts.pth` in `models/` directory
   - Or train your own model (see Training Your Own Model section)

### Running the Application

```bash
streamlit run app.py
```

Access at `http://localhost:8501`

## üíª Usage

### Face Recognition

1. **Enroll Users**:
   - Enter name in sidebar
   - Upload 1-3 clear face images
   - Click "Enroll"
   - Embeddings saved to `db/embeddings/{name}.npy`

2. **Recognize Faces**:
   - Click "Start" on camera widget
   - System displays:
     - üü¢ Green box: Recognized face with name & confidence
     - üî¥ Red box: Unknown face
   - Click "Stop" to end stream

### Emotion Detection

- **Automatic**: Runs simultaneously with face recognition
- **Display**: Emotion badge shown above detected face
- **Real-time**: Updates every 0.1s with temporal smoothing
- **7 Emotions**: Happy üòä, Sad üò¢, Angry üò†, Surprise üò≤, Fear üò®, Disgust ü§¢, Neutral üòê

## üéì Training Your Own Emotion Model

The project includes a complete training pipeline in `notebooks/Emotion.ipynb` for Google Colab.

> **Note**: This project initially used the [Face Expression Recognition Dataset](https://www.kaggle.com/datasets/jonathanoheix/face-expression-recognition-dataset) by Jonathan Oheix, but encountered accuracy issues with neutral/sad/angry emotions. We then retrained with [FER+ corrected labels](https://www.kaggle.com/datasets/ashishpatel26/fer2013) achieving 76.76% validation accuracy. See [Model Improvement Journey](#-model-improvement-journey) for the complete story.

### Quick Training Guide

1. **Open in Google Colab**:
   ```bash
   # Upload Emotion.ipynb to Google Colab
   # Or use: https://colab.research.google.com/
   ```

2. **Setup** (automated in notebook):
   - Mount Google Drive
   - Download FER2013 dataset from Kaggle (original or FER+ version)
   - Copy data to local disk (3x faster I/O)
   - Install dependencies

3. **Training Configuration** (already optimized):
   ```python
   # Model: SE-ResNet with channel attention
   model = ResNet(num_classes=7)
   
   # Loss: Focal Loss (handles class imbalance)
   loss = FocalLoss(alpha=class_weights, gamma=2.0, label_smoothing=0.1)
   
   # Optimizer: AdamW
   optimizer = AdamW(lr=1e-3, weight_decay=0.05)
   
   # Scheduler: OneCycleLR (25 epochs ~ 20-25 min on T4 GPU)
   scheduler = OneCycleLR(max_lr=1e-3, epochs=25, pct_start=0.1)
   
   # Batch size: 128
   # AMP: Enabled (mixed precision)
   ```

4. **Run Training**:
   - Execute cells sequentially
   - Training: ~40-60s/epoch on Colab T4 GPU
   - Total time: ~20-25 minutes
   - Checkpoints saved to Google Drive

5. **Export Model**:
   ```python
   # Convert to TorchScript (in notebook)
   model.eval()
   example = torch.randn(1, 1, 48, 48)
   ts_model = torch.jit.trace(model, example)
   ts_model = torch.jit.freeze(ts_model)
   ts_model.save("emotion_resnet_model_ts.pth")
   ```

6. **Download & Use**:
   - Download `.pth` file from Google Drive
   - Place in `models/` directory
   - Update path in `app.py` if needed

### Training Optimizations Applied

Our notebook includes production-ready optimizations:

| Optimization | Impact |
|--------------|--------|
| Local data copy (not Drive) | **3x faster I/O** |
| SE reduction=32 (vs 16) | **10% faster** |
| persistent_workers=True | **15% faster** |
| AMP (mixed precision) | **20% faster** |
| OneCycleLR (25 epochs vs 50) | **2x faster** |
| **Total speedup** | **~5-7x** |

**Colab Free Tier**: T4 GPU, 15GB VRAM, ~8 TFLOPS - sufficient for this project.

### Alternative: Kaggle

For faster training, use Kaggle instead of Colab:
- Free P100 GPU (~2x faster than T4)
- 30h/week compute time
- Better for longer experiments



## ‚öôÔ∏è Configuration

### Face Recognition Engine

```python
engine = FaceEngine(
    model_name="buffalo_s",              # buffalo_s (fast) or buffalo_l (accurate)
    providers=["CPUExecutionProvider"],  # CPU or CUDAExecutionProvider for GPU
    det_size=(640, 640),                 # Detection size (larger = more accurate, slower)
    db_dir="db/embeddings",              # Embedding storage directory
    threshold=0.40,                      # Recognition similarity threshold (0.0-1.0)
    unknown_percent_cutoff=30.0,         # Confidence cutoff for "Unknown"
)
```

### Emotion Detection Config

```python
class EmotionConfig:
    model_path = "models/emotion_resnet_model_ts.pth"
    
    # Buffer settings (multi-frame aggregation)
    buffer_size = 3                    # Number of frames to aggregate
    agg_method = "mean"                # mean, max, or voting
    
    # Smoothing
    use_buffer_ema = True              # Enable two-stage smoothing
    buffer_ema_alpha = 0.5             # EMA smoothing factor
    
    # Transition detection (adaptive sampling)
    snap_gap_threshold = 0.35          # High confidence threshold
    transition_gap_threshold = 0.10    # Medium confidence threshold
    
    # State switching
    min_streak_to_switch = 3           # Frames needed to change emotion
    snap_on_strong_new = True          # Immediate switch on very high confidence
    strong_new_gap = 0.35              # Gap for immediate switch
```

### Performance Tuning

```python
# In VideoProcessor class
self.inference_interval = 0.10         # Seconds between inference (10 FPS)
self.last_inference_time = 0.0         # Throttling for performance
```

**Tuning Guide**:
- `inference_interval`: 
  - Lower (0.05-0.10): More responsive, higher CPU usage
  - Higher (0.15-0.30): Better performance, slight delay
- `buffer_size`:
  - Smaller (2-3): Faster transitions, less smooth
  - Larger (5-7): Smoother, slower response
- `min_streak_to_switch`:
  - Lower (2): Faster emotion changes, more flicker risk
  - Higher (4-5): More stable, slower response



## ÔøΩ Known Limitations & Debug Mode

### FER2013 Dataset Limitations

The current emotion model was trained on FER2013, which has known labeling issues:

#### Documented Issues

1. **Neutral/Sad/Angry Confusion** (Most Common)
   - **Symptom**: Model predicts "sad" when face is actually neutral
   - **Root Cause**: FER2013 has mislabeled images in these 3 classes
   - **Evidence**: Confidence gaps between predictions are very low (0.02-0.20)
   - **Example**: Neutral face ‚Üí Model says "sad" with 60-70% confidence
   
2. **Happy Detection Works Well** ‚úÖ
   - **Performance**: 80-90% confidence gaps (excellent separation)
   - **Reason**: Happy is the most distinct emotion in the dataset

3. **Fear/Surprise Confusion**
   - **Symptom**: Fear often confused with surprise
   - **Reason**: Visual similarity in facial expressions

#### Why This Happens

FER2013 was originally crowdsourced with inconsistent labeling. Research papers report:
- **Angry/Sad/Neutral**: High inter-class confusion (15-25% error rate)
- **Happy/Surprise**: Relatively clean labels (<10% error rate)

### Debug Mode: Proving the Issue

To verify that the issue is **model training data** (not preprocessing), enable debug mode to inspect what the model sees:

**See**: [`debug_mode.md`](debug_mode.md) for complete instructions

**Quick Enable**:
1. Edit `app.py` line ~68: `debug_save_faces: bool = True`
2. Restart application
3. Check `debug_faces/` folder for saved 48√ó48 images

**Expected Result**: If images look good but predictions are wrong, it confirms FER2013 dataset limitation.

### Solutions

#### Short-term (Current)
- ‚úÖ **Temporal smoothing**: 3-tier pipeline reduces flicker
- ‚úÖ **Confidence filtering**: Reject predictions <55% confidence
- ‚úÖ **Gap filtering**: Require 25%+ separation between top-2 emotions
- ‚ö†Ô∏è **Accept limitation**: Document that neutral often shows as sad

#### Long-term (Recommended)
- üéØ **Retrain with FER+ dataset**: Corrected labels for FER2013 images
  - FER+ fixes ~40% of mislabeled images
  - Available on Kaggle: [FER+ Dataset](https://www.kaggle.com/datasets/ashishpatel26/fer2013)
  - Use same training notebook (`notebooks/Emotion.ipynb`)
  - Expected improvement: +5-10% accuracy on confused emotions

- üéØ **Use AffectNet dataset**: Larger, professionally labeled dataset
  - 440K images (vs 35K in FER2013)
  - More consistent labeling
  - Better generalization

### Performance Impact

| Emotion | FER2013 Confidence Gap | FER+ Expected |
|---------|----------------------|---------------|
| Happy | 0.80-0.90 ‚úÖ | 0.85-0.95 |
| Neutral | 0.02-0.15 ‚ùå | 0.40-0.60 |
| Sad | 0.05-0.20 ‚ùå | 0.35-0.50 |
| Angry | 0.10-0.25 ‚ùå | 0.40-0.55 |
| Surprise | 0.30-0.40 ‚ö†Ô∏è | 0.50-0.70 |
| Fear | 0.15-0.30 ‚ö†Ô∏è | 0.40-0.60 |

**Conclusion**: Current system works well for happy/surprise but struggles with neutral/sad/angry. Retraining with FER+ is recommended for production use.

---

## ÔøΩüîß Troubleshooting

### Camera Not Working

- **Browser Permissions**: Ensure browser allows camera access (check site settings)
- **Camera in Use**: Close other applications using camera (Zoom, Teams, etc.)
- **Refresh Browser**: Try refreshing page or restarting Streamlit server
- **HTTPS Required**: Some browsers require HTTPS for camera in production
- **Check Console**: Look for errors in browser developer console (F12)

### Low Recognition Accuracy

- **Enroll More Images**: Use 3-5 images per person for better accuracy
- **Good Lighting**: Ensure good lighting during enrollment and recognition
- **Frontal Faces**: Use frontal face images (avoid extreme angles)
- **Increase Detection Size**: `det_size=(640, 640)` ‚Üí `(1024, 1024)` for better accuracy
- **Better Model**: Use `buffalo_l` instead of `buffalo_s` for higher accuracy
- **Adjust Threshold**: Lower threshold (e.g., `threshold=0.35`) for more lenient matching
- **Image Quality**: Use high-resolution, clear images for enrollment

### Emotion Detection Issues

- **Flickering Emotions**: 
  - Increase `buffer_size` to 5-7 for more smoothing
  - Increase `min_streak_to_switch` to 4-5 for stability
  - Reduce `snap_gap_threshold` to 0.45 (less frequent snaps)
  
- **Always Same Emotion**:
  - Model may be overfitted or undertrained
  - Check confidence scores in logs
  - Verify model exists: `models/emotion_resnet_model_ts.pth`
  - Consider retraining with better augmentation

- **Neutral/Sad/Angry Confusion** ‚ö†Ô∏è **KNOWN ISSUE**:
  - **Root Cause**: FER2013 dataset mislabeling (see [Known Limitations](#-known-limitations--debug-mode))
  - **Debug**: Enable debug mode to verify preprocessing is correct ([debug_mode.md](debug_mode.md))
  - **Solution**: Retrain with FER+ dataset ([fer_plus_guide.md](fer_plus_guide.md))
  - **Expected**: 5-10% accuracy improvement, 0.40-0.60 confidence gaps (vs 0.02-0.20)
  
- **Poor Accuracy on Angry/Sad/Neutral** (General):
  - FER2013 dataset has inherent limitations (these emotions are easily confused)
  - Consider retraining with **FER+** or **AffectNet** dataset
  - Increase training epochs (30-50)
  - Use stronger augmentation

### Performance Issues

- **Reduce Detection Size**: `det_size=(640, 640)` ‚Üí `(320, 320)` or `(480, 480)`
- **Increase Inference Interval**: `self.inference_interval = 0.2` or higher
- **Reduce Max Faces**: `max_faces=2` ‚Üí `max_faces=1`
- **Faster Model**: Use `buffalo_s` instead of `buffalo_l`
- **CPU Optimization**: Ensure ONNX Runtime uses optimized CPU execution
- **Close Other Apps**: Free up system resources

### Module Import Errors

```bash
# Install all dependencies
pip install -r requirements.txt

# Or using uv (faster)
uv pip install -r requirements.txt

# Fix ONNX Runtime issues
pip install --upgrade onnxruntime

# Fix PyTorch issues
pip install --upgrade torch torchvision
```

For InsightFace model download issues, check internet connection and firewall settings.

### Performance Benchmarks

| Hardware | Detection | Recognition | Emotion | Total FPS |
|----------|-----------|-------------|---------|-----------|
| M1 Mac CPU | ~30ms | ~5ms | ~15ms | ~18-20 |
| Intel i7 CPU | ~45ms | ~8ms | ~20ms | ~12-15 |
| NVIDIA T4 GPU | ~10ms | ~2ms | ~8ms | ~40-50 |

*Note: Times include full pipeline (detection + recognition + emotion + smoothing)*

---

## üéØ Project Achievements

This project demonstrates several key machine learning and software engineering skills:

### 1. **Custom Deep Learning Model**
- Designed and trained SE-ResNet architecture from scratch
- Implemented Squeeze-and-Excitation attention mechanism for feature refinement
- Applied advanced training techniques: Focal Loss, OneCycleLR, Label Smoothing
- Achieved 5-7x training speedup through optimization (DataLoader tuning, local data caching)

### 2. **Advanced Temporal Smoothing Algorithm**
- Developed **three-tier smoothing pipeline** to solve real-world production issues:
  - **PredictionBuffer**: Multi-frame aggregation with adaptive sampling (SNAP/TRANSITION/FAMILY modes)
  - **BufferEMA**: Exponential moving average for continuous smoothing
  - **State Machine**: Streak-based confidence gating to prevent flicker
- Reduced emotion flickering while maintaining responsiveness

### 3. **Production-Ready ML Pipeline**
- Full training-to-deployment workflow:
  - Data preprocessing and augmentation (Albumentations)
  - Model training with mixed precision (AMP) and gradient clipping
  - TorchScript export for optimized inference
  - Real-time async inference at 10 FPS
- Optimized for Google Colab (T4 GPU, ~20-25 min training)

### 4. **Real-Time Computer Vision System**
- Multi-model pipeline: Face Detection ‚Üí Recognition ‚Üí Emotion Detection
- Asynchronous processing with frame throttling
- WebRTC video streaming with Streamlit
- Handles multiple faces with per-face emotion tracking

### 5. **Performance Engineering**
- Identified and fixed critical bugs (OneCycleLR scheduler misuse)
- Optimized DataLoader: `num_workers=2`, `persistent_workers=True`, `prefetch_factor=2`
- Local data caching in Colab for 3x I/O speedup
- Inference interval tuning for CPU/GPU balance

### 6. **Dataset Understanding**
- Analyzed FER2013 limitations (angry/sad/neutral confusion at 0.02-0.20 confidence gaps)
- Implemented targeted solutions: Class weighting, Focal Loss, aggressive smoothing
- Documented performance trade-offs and future improvements

---

## üöÄ Model Improvement Journey

This section documents the systematic process of improving model accuracy from initial deployment to production-ready performance.

### Initial Dataset Choice

**Started with**: [Face Expression Recognition Dataset](https://www.kaggle.com/datasets/jonathanoheix/face-expression-recognition-dataset) by Jonathan Oheix
- **Dataset**: Pre-processed FER2013 with train/test split
- **Initial Training**: Achieved ~65-70% validation accuracy
- **Deployment**: Model loaded successfully, inference working

### Problem Discovery

During initial testing with the Jonathan Oheix dataset model, discovered poor accuracy on neutral, sad, and angry emotions:
- **Symptom**: Neutral faces consistently detected as "sad" with 60-70% confidence
- **Impact**: Low confidence gaps (0.02-0.20) between top-2 predictions
- **User Experience**: Frequent emotion switching, low reliability
- **Root Cause Hypothesis**: Underlying FER2013 dataset quality issues

### Systematic Debugging Approach

**Step 1: Verify Preprocessing (Debug Mode)**
- **Action**: Implemented debug mode to save preprocessed 48√ó48 face images
  - Edit `app.py`: Set `debug_save_faces: bool = True`
  - Images saved to `debug_faces/` folder with prediction labels
- **Result**: Collected 112 debug images showing correct preprocessing ‚úÖ
- **Analysis**: Images were correctly grayscale, 48√ó48, properly aligned
- **Conclusion**: Issue was NOT in preprocessing pipeline

**Step 2: Analyze Model Predictions**
- **Tool**: Created `analyze_debug_faces.py` for batch analysis
- **Finding**: 83.9% of neutral faces incorrectly predicted as "sad"
- **Pattern**: Low confidence gaps between competing emotions (0.05-0.15)
- **Root Cause Identified**: FER2013 dataset has ~40% mislabeling in neutral/sad/angry classes


### Critical Bug Discovery

During FER+ retraining preparation, discovered a **catastrophic label mapping bug**:

```python
# WRONG (Original Code) - Shuffled emotion order
votes = [
    ferplus_row['neutral'],     # 0 - WRONG!
    ferplus_row['happiness'],   # 1 - WRONG!
    # ... incorrect mapping
]

# CORRECT (Fixed Code) - Matches FER2013 indices
votes = [
    ferplus_row['anger'],      # 0 - angry ‚úÖ
    ferplus_row['disgust'],    # 1 - disgust ‚úÖ
    ferplus_row['fear'],       # 2 - fear ‚úÖ
    ferplus_row['happiness'],  # 3 - happy ‚úÖ
    ferplus_row['neutral'],    # 4 - neutral ‚úÖ
    ferplus_row['sadness'],    # 5 - sad ‚úÖ
    ferplus_row['surprise']    # 6 - surprise ‚úÖ
]
```

**Impact**: Training accuracy **1.24%** (completely broken!)

**Fix**: Corrected emotion label order to match FER2013 standard mapping

### FER+ Dataset Retraining

**Decision to Switch Datasets**:
- Initial dataset (Jonathan Oheix) is based on FER2013 with known labeling issues
- Research showed FER+ provides corrected labels via crowd-sourcing
- Goal: Improve accuracy on problematic emotion classes (neutral/sad/angry)

**Dataset Acquisition**:
1. Downloaded `fer2013.csv` (301MB) - original images
2. Downloaded `fer2013new.csv` - corrected labels from [FER+ project](https://github.com/microsoft/FERPlus)
3. Uploaded to Google Drive for Colab access

**FER+ Advantages over Original FER2013**:
- **10+ annotators per image** (vs 1 in original FER2013)
- **Majority vote labels** - more reliable ground truth
- **Filtered ambiguous samples** - images with no clear consensus removed
- **Fixes ~40% of mislabeled images** in neutral/sad/angry classes
- **Same images, better labels** - direct comparison possible

**Training Results** (Google Colab T4 GPU):
```
Epoch 23/25: Train Loss=0.420, Val Loss=0.749, Val Acc=76.76% ‚≠ê BEST
Epoch 24/25: Train Loss=0.406, Val Loss=0.754, Val Acc=76.32%
Epoch 25/25: Train Loss=0.394, Val Loss=0.761, Val Acc=76.08%
```

**Model Export**:
- Saved as: `emotion_resnet_fer_model_ts.pth` (43MB)
- Format: TorchScript for optimized inference
- Architecture: SE-ResNet with reduction=32

**Accuracy Improvement**:
| Metric | Before (Bug) | After (FER+ Fixed) | Improvement |
|--------|-------------|-------------------|-------------|
| Validation Accuracy | **1.24%** ‚ùå | **76.76%** ‚úÖ | **+75.52%** |
| Neutral Detection | Poor (83.9% wrong) | Good (correct predictions) | Significant |
| Confidence Gaps | 0.02-0.20 (low) | 0.03-0.08 (FER+ typical) | More decisive |

### UI Responsiveness Optimization

After deploying the FER+ model, discovered UI wasn't updating smoothly despite correct backend predictions.

**Problem**:
- **Logs**: Correct emotion transitions detected (`[STATE] Emotion: neutral ‚Üí happy`)
- **UI**: Emotion icon not updating
- **Root Cause**: Switching logic too conservative for FER+ model's prediction characteristics

**Analysis**:
- Original settings designed for FER2013 model (large confidence gaps: 0.10-0.30)
- FER+ model produces **smaller gaps** (0.03-0.08) due to better label quality
- Gap filter rejecting 80%+ of valid predictions: `[REJECT] Low gap: neutral gap=0.058 < 0.06`

**Tuning Process**:

1. **First Iteration - Reduce Strictness**:
   ```python
   # Before
   switch_streak_required: int = 5      # Too slow
   switch_min_gap: float = 0.30         # Too high
   
   # After
   switch_streak_required: int = 2      # Faster response
   switch_min_gap: float = 0.08         # Lower barrier
   ```
   **Result**: Improved but still many rejections

2. **Second Iteration - Disable Gap Filter**:
   ```python
   # Final Configuration
   use_gap_filter: bool = False         # DISABLED
   min_gap: float = 0.03                # Backup threshold
   switch_streak_required: int = 2      # Keep fast response
   ```
   **Result**: Smooth UI updates ‚úÖ

**Key Insight**: FER+ model's smaller confidence gaps are actually a **feature, not a bug**. They indicate:
- More balanced class distributions (better training data)
- More realistic emotion predictions (faces often show mixed emotions)
- Need for different filtering strategy than high-gap models

**User Feedback**: "It seems smoothly more than preview" ‚úÖ

### Final Configuration (Production)

```python
@dataclass
class EmotionConfig:
    model_path: str = "models/emotion_resnet_fer_model_ts.pth"  # FER+ trained model
    min_confidence: float = 0.30          # 30% threshold
    use_gap_filter: bool = False          # Disabled for FER+ model
    min_gap: float = 0.03                 # Backup threshold
    switch_streak_required: int = 2       # Fast emotion changes
    switch_min_gap: float = 0.08          # Reasonable switching barrier
    debug_save_faces: bool = True         # Keep for future debugging
```

### Lessons Learned

1. **Always verify label mappings** when using external datasets
2. **Debug systematically**: Preprocessing ‚Üí Model ‚Üí Postprocessing
3. **Different models need different tuning**: FER+ vs FER2013 have distinct characteristics
4. **Document everything**: Debug process helps future troubleshooting
5. **Test with real users**: Logs showed correct predictions, but UI smoothness matters

### Key Metrics
- **Model Accuracy**: **76.76%** on FER+ validation set (best epoch 23/25)
  - Previous (FER2013): ~65-70%
  - Improvement: **+6-11%** from better training data
- **Label Bug Impact**: Fixed catastrophic bug (1.24% ‚Üí 76.76% accuracy)
- **Training Time**: 40-60s/epoch on T4 GPU (25 epochs = 20-25 min)
- **Inference Speed**: 18-20 FPS on M1 Mac CPU, 40-50 FPS on T4 GPU
- **UI Responsiveness**: Smooth emotion updates after tuning (streak=2, gap filter disabled)
- **Confidence Gaps** (FER+ model):
  - Happy: 0.30-0.50 (good separation)
  - Neutral/Sad/Angry: 0.03-0.08 (smaller but more accurate)
  - Overall: More balanced predictions reflecting real-world emotion complexity

---

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is open source and available under the MIT License.

## Acknowledgments

- **[InsightFace](https://github.com/deepinsight/insightface)** - State-of-the-art face recognition models and framework
- **[Streamlit](https://streamlit.io/)** - Modern web framework for data applications
- **[streamlit-webrtc](https://github.com/whitphx/streamlit-webrtc)** - Real-time WebRTC video streaming for Streamlit
- **[ONNX Runtime](https://onnxruntime.ai/)** - High-performance inference engine
- **[FER2013 Dataset](https://www.kaggle.com/datasets/msambare/fer2013)** - Facial expression recognition dataset
- **[PyTorch](https://pytorch.org/)** - Deep learning framework

## üìö Further Reading

- [InsightFace Documentation](https://insightface.ai/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [SE-Net Paper (Hu et al., 2018)](https://arxiv.org/abs/1709.01507) - Squeeze-and-Excitation Networks
- [Focal Loss Paper (Lin et al., 2017)](https://arxiv.org/abs/1708.02002) - Focal Loss for Dense Object Detection
- [OneCycleLR Paper (Smith, 2018)](https://arxiv.org/abs/1803.09820) - Super-Convergence: Very Fast Training of Neural Networks

## üì¨ Contact

For questions, suggestions, or support, please open an issue in the GitHub repository.

---

**Built with ‚ù§Ô∏è using PyTorch, InsightFace, and Streamlit**

